{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook outlining what functions from nltk will be useful for pre-processing the text and training the classification algorithm.\n",
    "\n",
    "Expects a CSV file in the current directory containing a release's date, text, and classification (pos or neg) in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.decomposition\n",
    "\n",
    "from sklearn import decomposition, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pre-processing to get the dataframe into the format we need for content analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`allreleasescleaned.csv` contains all press releases from 1994 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histreleasesdf = pd.read_csv('allreleasescleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>release_text</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>November 15, 1994</td>\n",
       "      <td>The Federal Reserve Board today approved an in...</td>\n",
       "      <td>1994</td>\n",
       "      <td>November</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>August 16, 1994</td>\n",
       "      <td>The Federal Reserve announced today the follow...</td>\n",
       "      <td>1994</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>May 17, 1994</td>\n",
       "      <td>The Federal Reserve today announced two action...</td>\n",
       "      <td>1994</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>April 18, 1994</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "      <td>1994</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>March 22, 1994</td>\n",
       "      <td>Chairman Alan Greenspan announced today that ...</td>\n",
       "      <td>1994</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date                                       release_text  year  \\\n",
       "0  November 15, 1994  The Federal Reserve Board today approved an in...  1994   \n",
       "1    August 16, 1994  The Federal Reserve announced today the follow...  1994   \n",
       "2       May 17, 1994  The Federal Reserve today announced two action...  1994   \n",
       "3     April 18, 1994  Chairman Alan Greenspan announced today that t...  1994   \n",
       "4     March 22, 1994   Chairman Alan Greenspan announced today that ...  1994   \n",
       "\n",
       "      month  \n",
       "0  November  \n",
       "1    August  \n",
       "2       May  \n",
       "3     April  \n",
       "4     March  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histreleasesdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`allratesdf.csv` contains cleaned DataFrame of all relevant rate changes from 1994."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allratesdf = pd.read_csv('allratesdf.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Change</th>\n",
       "      <th>increase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>February</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.07</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.03</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.07</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>June</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.06</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  year  Change  increase\n",
       "0  February  1994    0.07      True\n",
       "1     March  1994    0.03      True\n",
       "2     April  1994    0.07      True\n",
       "3       May  1994    0.13      True\n",
       "4      June  1994    0.06      True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allratesdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to merge the text and rates DataFrames and clean the result. `allreleaserates.csv` has all rate changes since 1994 and their associated release text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now text processing using `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "releaserates_df = pd.read_csv('allreleaserates.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Change</th>\n",
       "      <th>increase</th>\n",
       "      <th>date</th>\n",
       "      <th>release_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>February</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.07</td>\n",
       "      <td>True</td>\n",
       "      <td>February 4, 1994</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>March</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.03</td>\n",
       "      <td>True</td>\n",
       "      <td>March 22, 1994</td>\n",
       "      <td>Chairman Alan Greenspan announced today that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.07</td>\n",
       "      <td>True</td>\n",
       "      <td>April 18, 1994</td>\n",
       "      <td>Chairman Alan Greenspan announced today that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.13</td>\n",
       "      <td>True</td>\n",
       "      <td>May 17, 1994</td>\n",
       "      <td>The Federal Reserve today announced two action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>June</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.06</td>\n",
       "      <td>True</td>\n",
       "      <td>May 17, 1994</td>\n",
       "      <td>The Federal Reserve today announced two action...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  year  Change  increase              date  \\\n",
       "0  February  1994    0.07      True  February 4, 1994   \n",
       "1     March  1994    0.03      True    March 22, 1994   \n",
       "2     April  1994    0.07      True    April 18, 1994   \n",
       "3       May  1994    0.13      True      May 17, 1994   \n",
       "4      June  1994    0.06      True      May 17, 1994   \n",
       "\n",
       "                                        release_text  \n",
       "0  Chairman Alan Greenspan announced today that t...  \n",
       "1   Chairman Alan Greenspan announced today that ...  \n",
       "2  Chairman Alan Greenspan announced today that t...  \n",
       "3  The Federal Reserve today announced two action...  \n",
       "4  The Federal Reserve today announced two action...  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "releaserates_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we try a Logistic Regression using PCA on tf-idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(releaserates_df['release_text'], releaserates_df['increase'], test_size = .30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "releaseTFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.5, min_df=3, stop_words='english', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "releaseTFVects = releaseTFVectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If interested in the scores assigned to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sorted(list(zip(releaseTFVectorizer.vocabulary_.keys(), releaseTFVects.data)), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCA = decomposition.PCA\n",
    "pca = PCA().fit(releaseTFVects.toarray())\n",
    "reduced_data = pca.transform(releaseTFVects.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Scree Plot indicating the proportion of variance explained by the first $x$ principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAFNCAYAAAA+ZchVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115e4f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = releaseTFVects.shape[0]\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "eigen_vals = np.arange(n) + 1\n",
    "ax1.plot(eigen_vals, pca.explained_variance_ratio_, 'ro-', linewidth=2)\n",
    "ax1.set_title('Scree Plot')\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Proportion of Explained Variance')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "eigen_vals = np.arange(20) + 1\n",
    "ax2.plot(eigen_vals, pca.explained_variance_ratio_[:20], 'ro-', linewidth=2)\n",
    "ax2.set_title('Scree Plot (First 20 Principal Components)')\n",
    "ax2.set_xlabel('Principal Component')\n",
    "ax2.set_ylabel('Proportion of Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first try a Logistic Regression using the first 10 principal components as regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :10]\n",
    "Y = np.array([int(label) for label in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training set: 0.6536585365853659\n"
     ]
    }
   ],
   "source": [
    "print('For training set: {}'.format(logistic.score(X,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set: 0.550561797752809\n"
     ]
    }
   ],
   "source": [
    "releaseTFVects_test = releaseTFVectorizer.transform(x_test)\n",
    "reduced_data_test = pca.transform(releaseTFVects_test.toarray())\n",
    "X_test = reduced_data_test[:, :10]\n",
    "Y_test = np.array([int(label) for label in y_test])\n",
    "print('For testing set: {}'.format(logistic.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about 21 principal components? (Seems like a reasonable knee in the scree plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reduced_data[:, :21]\n",
    "Y = np.array([int(label) for label in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training set: 0.7365853658536585\n"
     ]
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X, Y)\n",
    "print('For training set: {}'.format(logistic.score(X,Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For testing set: 0.5617977528089888\n"
     ]
    }
   ],
   "source": [
    "releaseTFVects_test = releaseTFVectorizer.transform(x_test)\n",
    "reduced_data_test = pca.transform(releaseTFVects_test.toarray())\n",
    "X_test = reduced_data_test[:, :21]\n",
    "Y_test = np.array([int(label) for label in y_test])\n",
    "print('For testing set: {}'.format(logistic.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this point, more components doesn't result in much of an increase in predictive power. But, this still isn't great. To solve this problem, we should try different classifying algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>Change</th>\n",
       "      <th>increase</th>\n",
       "      <th>date</th>\n",
       "      <th>release_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>January</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.42</td>\n",
       "      <td>True</td>\n",
       "      <td>January 27, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>February</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.12</td>\n",
       "      <td>True</td>\n",
       "      <td>January 27, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>March</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>False</td>\n",
       "      <td>March 16, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>April</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.03</td>\n",
       "      <td>True</td>\n",
       "      <td>April 27, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>May</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>April 27, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>June</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.03</td>\n",
       "      <td>True</td>\n",
       "      <td>June 15, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>July</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.03</td>\n",
       "      <td>True</td>\n",
       "      <td>July 27, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>August</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.03</td>\n",
       "      <td>True</td>\n",
       "      <td>July 27, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>September</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>September 21, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>October</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>September 21, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>November</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.02</td>\n",
       "      <td>True</td>\n",
       "      <td>November 02, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>December</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.32</td>\n",
       "      <td>True</td>\n",
       "      <td>December 14, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>January</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.20</td>\n",
       "      <td>True</td>\n",
       "      <td>December 14, 2016</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>February</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.02</td>\n",
       "      <td>True</td>\n",
       "      <td>February 01, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>March</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.20</td>\n",
       "      <td>True</td>\n",
       "      <td>March 15, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>April</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.14</td>\n",
       "      <td>True</td>\n",
       "      <td>March 15, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>May</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>May 03, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>June</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.14</td>\n",
       "      <td>True</td>\n",
       "      <td>June 14, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>July</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.11</td>\n",
       "      <td>True</td>\n",
       "      <td>July 26, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>August</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>July 26, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>September</td>\n",
       "      <td>2017</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>September 20, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>October</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.00</td>\n",
       "      <td>False</td>\n",
       "      <td>September 20, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>November 01, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>December</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.12</td>\n",
       "      <td>True</td>\n",
       "      <td>December 13, 2017</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>January</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.08</td>\n",
       "      <td>True</td>\n",
       "      <td>January 31, 2018</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         month  year  Change  increase                date  \\\n",
       "269    January  2016    0.42      True    January 27, 2016   \n",
       "270   February  2016    0.12      True    January 27, 2016   \n",
       "271      March  2016   -0.05     False      March 16, 2016   \n",
       "272      April  2016    0.03      True      April 27, 2016   \n",
       "273        May  2016    0.00     False      April 27, 2016   \n",
       "274       June  2016    0.03      True       June 15, 2016   \n",
       "275       July  2016    0.03      True       July 27, 2016   \n",
       "276     August  2016    0.03      True       July 27, 2016   \n",
       "277  September  2016    0.00     False  September 21, 2016   \n",
       "278    October  2016    0.00     False  September 21, 2016   \n",
       "279   November  2016    0.02      True   November 02, 2016   \n",
       "280   December  2016    0.32      True   December 14, 2016   \n",
       "281    January  2017    0.20      True   December 14, 2016   \n",
       "282   February  2017    0.02      True   February 01, 2017   \n",
       "283      March  2017    0.20      True      March 15, 2017   \n",
       "284      April  2017    0.14      True      March 15, 2017   \n",
       "285        May  2017    0.01      True        May 03, 2017   \n",
       "286       June  2017    0.14      True       June 14, 2017   \n",
       "287       July  2017    0.11      True       July 26, 2017   \n",
       "288     August  2017    0.01      True       July 26, 2017   \n",
       "289  September  2017   -0.01     False  September 20, 2017   \n",
       "290    October  2017    0.00     False  September 20, 2017   \n",
       "291   November  2017    0.01      True   November 01, 2017   \n",
       "292   December  2017    0.12      True   December 13, 2017   \n",
       "293    January  2018    0.08      True    January 31, 2018   \n",
       "\n",
       "                                          release_text  \n",
       "269  Information received since the Federal Open Ma...  \n",
       "270  Information received since the Federal Open Ma...  \n",
       "271  Information received since the Federal Open Ma...  \n",
       "272  Information received since the Federal Open Ma...  \n",
       "273  Information received since the Federal Open Ma...  \n",
       "274  Information received since the Federal Open Ma...  \n",
       "275  Information received since the Federal Open Ma...  \n",
       "276  Information received since the Federal Open Ma...  \n",
       "277  Information received since the Federal Open Ma...  \n",
       "278  Information received since the Federal Open Ma...  \n",
       "279  Information received since the Federal Open Ma...  \n",
       "280  Information received since the Federal Open Ma...  \n",
       "281  Information received since the Federal Open Ma...  \n",
       "282  Information received since the Federal Open Ma...  \n",
       "283  Information received since the Federal Open Ma...  \n",
       "284  Information received since the Federal Open Ma...  \n",
       "285  Information received since the Federal Open Ma...  \n",
       "286  Information received since the Federal Open Ma...  \n",
       "287  Information received since the Federal Open Ma...  \n",
       "288  Information received since the Federal Open Ma...  \n",
       "289  Information received since the Federal Open Ma...  \n",
       "290  Information received since the Federal Open Ma...  \n",
       "291  Information received since the Federal Open Ma...  \n",
       "292  Information received since the Federal Open Ma...  \n",
       "293  Information received since the Federal Open Ma...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#releaserates_df.iloc[125:]\n",
    "releaserates_df.iloc[269:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(releaserates_df['release_text'], releaserates_df['increase'], test_size = .30, random_state = 42)\n",
    "#train_df = pd.DataFrame({'release_text':x_train, 'increase':y_train})\n",
    "#test_df = pd.DataFrame({'release_text':x_test, 'increase' : y_test})\n",
    "\n",
    "# settings for testing on releases after Jan 2017 (with data since 2006)\n",
    "#train_df = releaserates_df.iloc[:137].copy()\n",
    "#test_df = releaserates_df.iloc[137:].copy()\n",
    "\n",
    "# with data since 1994\n",
    "#train_df = releaserates_df.iloc[:281].copy()\n",
    "#test_df = releaserates_df.iloc[281:].copy()\n",
    "\n",
    "# settings for testing on releases after Jan 2016 (with data since 2006)\n",
    "#train_df = releaserates_df.iloc[:125].copy()\n",
    "#test_df = releaserates_df.iloc[125:].copy()\n",
    "\n",
    "# with data since 1994\n",
    "train_df = releaserates_df.iloc[:269].copy()\n",
    "test_df = releaserates_df.iloc[269:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Try Leave One Out testing [DONE]\n",
    "- Better model descriptions\n",
    "- Include past releases [DONE]\n",
    "- Include unemployment as another (or comparison) variable\n",
    "- Determine most characteristic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.naive_bayes.GaussianNB()\n",
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=15, stop_words='english', norm='l2')\n",
    "\n",
    "TFVects = TFVectorizer.fit_transform(train_df['release_text'])\n",
    "train_df['vect'] = [np.array(v).flatten() for v in TFVects.todense()]\n",
    "\n",
    "TFVects_test = TFVectorizer.transform(test_df['release_text'])\n",
    "test_df['vect'] = [np.array(v).flatten() for v in TFVects_test.todense()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naiveBayes = sklearn.naive_bayes.BernoulliNB()\n",
    "naiveBayes.fit(np.stack(train_df['vect'], axis=0), train_df['increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.8113879003558719\n",
      "Testing:\n",
      "0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\")\n",
    "print(naiveBayes.score(np.stack(train_df['vect'], axis=0), train_df['increase']))\n",
    "print(\"Testing:\")\n",
    "print(naiveBayes.score(np.stack(test_df['vect'], axis=0), test_df['increase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.00      0.00      0.00         2\n",
      "       True       0.85      1.00      0.92        11\n",
      "\n",
      "avg / total       0.72      0.85      0.78        13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephdenby/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = naiveBayes.predict(np.stack(test_df['vect'], axis=0))\n",
    "print(classification_report(test_df['increase'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tree = sklearn.tree.DecisionTreeClassifier(max_depth=4,random_state=0)\n",
    "clf_tree.fit(np.stack(train_df['vect'], axis =0), train_df['increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.5871886120996441\n",
      "Testing:\n",
      "0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\")\n",
    "print(sklearn.metrics.accuracy_score(train_df['increase'],clf_tree.predict(np.stack(train_df['vect'], axis = 0))))\n",
    "print(\"Testing:\")\n",
    "print(sklearn.metrics.accuracy_score(test_df['increase'],clf_tree.predict(np.stack(test_df['vect'], axis = 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.00      0.00      0.00         2\n",
      "       True       0.85      1.00      0.92        11\n",
      "\n",
      "avg / total       0.72      0.85      0.78        13\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephdenby/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_tree.predict(np.stack(test_df['vect'], axis=0))\n",
    "print(classification_report(test_df['increase'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = sklearn.tree.DecisionTreeClassifier(max_depth=10) #Create an instance of our decision tree classifier.\n",
    "\n",
    "bag = sklearn.ensemble.BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1) #Each tree uses up to 80% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=0.8, n_estimators=100, n_jobs=1, oob_score=False,\n",
       "         random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.fit(np.stack(train_df['vect'], axis =0), train_df['increase']) #Fit the bagged classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "0.8438661710037175\n",
      "Testing:\n",
      "0.24\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\")\n",
    "print(bag.score(np.stack(train_df['vect'], axis=0), train_df['increase']))\n",
    "print(\"Testing:\")\n",
    "print(bag.score(np.stack(test_df['vect'], axis=0), test_df['increase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.19      0.67      0.30         6\n",
      "       True       0.50      0.11      0.17        19\n",
      "\n",
      "avg / total       0.43      0.24      0.20        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag.predict(np.stack(test_df['vect'], axis=0))\n",
    "print(classification_report(test_df['increase'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "weights=\"uniform\"\n",
    "clf_knearest = sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=15, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_knearest.fit(np.stack(train_df['vect'], axis = 0), train_df['increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "0.6725978647686833\n",
      "Testing: \n",
      "0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "print('Training: ')\n",
    "print(clf_knearest.score(np.stack(train_df['vect'], axis=0), train_df['increase']))\n",
    "print('Testing: ')\n",
    "print(clf_knearest.score(np.stack(test_df['vect'], axis=0), test_df['increase']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = sklearn.svm.SVC(kernel='linear', probability = False)\n",
    "clf_svm.fit(np.stack(train_df['vect'], axis=0), train_df['increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "0.8256227758007118\n",
      "Testing: \n",
      "0.8461538461538461\n"
     ]
    }
   ],
   "source": [
    "print('Training: ')\n",
    "print(clf_svm.score(np.stack(train_df['vect'], axis=0), train_df['increase']))\n",
    "print('Testing: ')\n",
    "print(clf_svm.score(np.stack(test_df['vect'], axis=0), test_df['increase']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = sklearn.svm.SVC(kernel = 'poly', degree = 3, probability = False)\n",
    "clf_svm.fit(np.stack(train_df['vect'], axis=0), train_df['increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "0.501779359430605\n",
      "Testing: \n",
      "0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "print('Training: ')\n",
    "print(clf_svm.score(np.stack(train_df['vect'], axis=0), train_df['increase']))\n",
    "print('Testing: ')\n",
    "print(clf_svm.score(np.stack(test_df['vect'], axis=0), test_df['increase']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEURAL NETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nn = sklearn.neural_network.MLPClassifier()\n",
    "clf_nn.fit(np.stack(train_df['vect'], axis=0), train_df['increase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: \n",
      "0.8661710037174721\n",
      "Testing: \n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "print('Training: ')\n",
    "print(clf_nn.score(np.stack(train_df['vect'], axis=0), train_df['increase']))\n",
    "print('Testing: ')\n",
    "print(clf_nn.score(np.stack(test_df['vect'], axis=0), test_df['increase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       1.00      0.33      0.50         6\n",
      "       True       0.83      1.00      0.90        19\n",
      "\n",
      "avg / total       0.87      0.84      0.81        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_nn.predict(np.stack(test_df['vect'], axis=0))\n",
    "print(classification_report(test_df['increase'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like using a Neural Net or Bagging model is our best bet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFVectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_df=15, stop_words='english', norm='l2')\n",
    "\n",
    "TFVects = TFVectorizer.fit_transform(releaserates_df['release_text'])\n",
    "\n",
    "releaserates_df['vect'] = [np.array(v).flatten() for v in TFVects.todense()]\n",
    "\n",
    "yvals = releaserates_df['increase'].apply(lambda x: int(x)).values\n",
    "Xvals = releaserates_df['vect'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set [0]  is [0.]\n",
      "MSE for test set [1]  is [0.]\n",
      "MSE for test set [2]  is [0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephdenby/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set [3]  is [0.]\n",
      "MSE for test set [4]  is [0.]\n",
      "MSE for test set [5]  is [0.]\n",
      "MSE for test set [6]  is [0.]\n",
      "MSE for test set [7]  is [0.]\n",
      "MSE for test set [8]  is [0.]\n",
      "MSE for test set [9]  is [0.]\n",
      "MSE for test set [10]  is [0.]\n",
      "MSE for test set [11]  is [0.]\n",
      "MSE for test set [12]  is [1.]\n",
      "MSE for test set [13]  is [1.]\n",
      "MSE for test set [14]  is [1.]\n",
      "MSE for test set [15]  is [1.]\n",
      "MSE for test set [16]  is [1.]\n",
      "MSE for test set [17]  is [0.]\n",
      "MSE for test set [18]  is [1.]\n",
      "MSE for test set [19]  is [1.]\n",
      "MSE for test set [20]  is [0.]\n",
      "MSE for test set [21]  is [1.]\n",
      "MSE for test set [22]  is [0.]\n",
      "MSE for test set [23]  is [0.]\n",
      "MSE for test set [24]  is [0.]\n",
      "MSE for test set [25]  is [1.]\n",
      "MSE for test set [26]  is [1.]\n",
      "MSE for test set [27]  is [1.]\n",
      "MSE for test set [28]  is [1.]\n",
      "MSE for test set [29]  is [1.]\n",
      "MSE for test set [30]  is [0.]\n",
      "MSE for test set [31]  is [1.]\n",
      "MSE for test set [32]  is [0.]\n",
      "MSE for test set [33]  is [1.]\n",
      "MSE for test set [34]  is [0.]\n",
      "MSE for test set [35]  is [0.]\n",
      "MSE for test set [36]  is [0.]\n",
      "MSE for test set [37]  is [1.]\n",
      "MSE for test set [38]  is [1.]\n",
      "MSE for test set [39]  is [1.]\n",
      "MSE for test set [40]  is [1.]\n",
      "MSE for test set [41]  is [1.]\n",
      "MSE for test set [42]  is [1.]\n",
      "MSE for test set [43]  is [1.]\n",
      "MSE for test set [44]  is [1.]\n",
      "MSE for test set [45]  is [1.]\n",
      "MSE for test set [46]  is [1.]\n",
      "MSE for test set [47]  is [1.]\n",
      "MSE for test set [48]  is [1.]\n",
      "MSE for test set [49]  is [1.]\n",
      "MSE for test set [50]  is [1.]\n",
      "MSE for test set [51]  is [1.]\n",
      "MSE for test set [52]  is [1.]\n",
      "MSE for test set [53]  is [1.]\n",
      "MSE for test set [54]  is [1.]\n",
      "MSE for test set [55]  is [1.]\n",
      "MSE for test set [56]  is [0.]\n",
      "MSE for test set [57]  is [0.]\n",
      "MSE for test set [58]  is [0.]\n",
      "MSE for test set [59]  is [0.]\n",
      "MSE for test set [60]  is [1.]\n",
      "MSE for test set [61]  is [1.]\n",
      "MSE for test set [62]  is [0.]\n",
      "MSE for test set [63]  is [1.]\n",
      "MSE for test set [64]  is [0.]\n",
      "MSE for test set [65]  is [0.]\n",
      "MSE for test set [66]  is [0.]\n",
      "MSE for test set [67]  is [0.]\n",
      "MSE for test set [68]  is [1.]\n",
      "MSE for test set [69]  is [0.]\n",
      "MSE for test set [70]  is [1.]\n",
      "MSE for test set [71]  is [1.]\n",
      "MSE for test set [72]  is [0.]\n",
      "MSE for test set [73]  is [0.]\n",
      "MSE for test set [74]  is [0.]\n",
      "MSE for test set [75]  is [0.]\n",
      "MSE for test set [76]  is [0.]\n",
      "MSE for test set [77]  is [0.]\n",
      "MSE for test set [78]  is [1.]\n",
      "MSE for test set [79]  is [1.]\n",
      "MSE for test set [80]  is [1.]\n",
      "MSE for test set [81]  is [0.]\n",
      "MSE for test set [82]  is [0.]\n",
      "MSE for test set [83]  is [0.]\n",
      "MSE for test set [84]  is [0.]\n",
      "MSE for test set [85]  is [0.]\n",
      "MSE for test set [86]  is [0.]\n",
      "MSE for test set [87]  is [0.]\n",
      "MSE for test set [88]  is [0.]\n",
      "MSE for test set [89]  is [0.]\n",
      "MSE for test set [90]  is [0.]\n",
      "MSE for test set [91]  is [0.]\n",
      "MSE for test set [92]  is [0.]\n",
      "MSE for test set [93]  is [0.]\n",
      "MSE for test set [94]  is [0.]\n",
      "MSE for test set [95]  is [0.]\n",
      "MSE for test set [96]  is [1.]\n",
      "MSE for test set [97]  is [1.]\n",
      "MSE for test set [98]  is [1.]\n",
      "MSE for test set [99]  is [1.]\n",
      "MSE for test set [100]  is [0.]\n",
      "MSE for test set [101]  is [0.]\n",
      "MSE for test set [102]  is [0.]\n",
      "MSE for test set [103]  is [1.]\n",
      "MSE for test set [104]  is [1.]\n",
      "MSE for test set [105]  is [1.]\n",
      "MSE for test set [106]  is [0.]\n",
      "MSE for test set [107]  is [0.]\n",
      "MSE for test set [108]  is [1.]\n",
      "MSE for test set [109]  is [1.]\n",
      "MSE for test set [110]  is [1.]\n",
      "MSE for test set [111]  is [1.]\n",
      "MSE for test set [112]  is [0.]\n",
      "MSE for test set [113]  is [0.]\n",
      "MSE for test set [114]  is [0.]\n",
      "MSE for test set [115]  is [1.]\n",
      "MSE for test set [116]  is [0.]\n",
      "MSE for test set [117]  is [0.]\n",
      "MSE for test set [118]  is [0.]\n",
      "MSE for test set [119]  is [1.]\n",
      "MSE for test set [120]  is [0.]\n",
      "MSE for test set [121]  is [0.]\n",
      "MSE for test set [122]  is [0.]\n",
      "MSE for test set [123]  is [0.]\n",
      "MSE for test set [124]  is [1.]\n",
      "MSE for test set [125]  is [0.]\n",
      "MSE for test set [126]  is [0.]\n",
      "MSE for test set [127]  is [0.]\n",
      "MSE for test set [128]  is [0.]\n",
      "MSE for test set [129]  is [0.]\n",
      "MSE for test set [130]  is [0.]\n",
      "MSE for test set [131]  is [0.]\n",
      "MSE for test set [132]  is [0.]\n",
      "MSE for test set [133]  is [0.]\n",
      "MSE for test set [134]  is [0.]\n",
      "MSE for test set [135]  is [0.]\n",
      "MSE for test set [136]  is [0.]\n",
      "MSE for test set [137]  is [0.]\n",
      "MSE for test set [138]  is [0.]\n",
      "MSE for test set [139]  is [0.]\n",
      "MSE for test set [140]  is [0.]\n",
      "MSE for test set [141]  is [0.]\n",
      "MSE for test set [142]  is [0.]\n",
      "MSE for test set [143]  is [0.]\n",
      "MSE for test set [144]  is [0.]\n",
      "MSE for test set [145]  is [0.]\n",
      "MSE for test set [146]  is [0.]\n",
      "MSE for test set [147]  is [0.]\n",
      "MSE for test set [148]  is [0.]\n",
      "MSE for test set [149]  is [0.]\n",
      "MSE for test set [150]  is [0.]\n",
      "MSE for test set [151]  is [1.]\n",
      "MSE for test set [152]  is [0.]\n",
      "MSE for test set [153]  is [0.]\n",
      "MSE for test set [154]  is [0.]\n",
      "MSE for test set [155]  is [0.]\n",
      "MSE for test set [156]  is [0.]\n",
      "MSE for test set [157]  is [0.]\n",
      "MSE for test set [158]  is [0.]\n",
      "MSE for test set [159]  is [0.]\n",
      "MSE for test set [160]  is [0.]\n",
      "MSE for test set [161]  is [1.]\n",
      "MSE for test set [162]  is [1.]\n",
      "MSE for test set [163]  is [0.]\n",
      "MSE for test set [164]  is [0.]\n",
      "MSE for test set [165]  is [0.]\n",
      "MSE for test set [166]  is [0.]\n",
      "MSE for test set [167]  is [0.]\n",
      "MSE for test set [168]  is [0.]\n",
      "MSE for test set [169]  is [0.]\n",
      "MSE for test set [170]  is [0.]\n",
      "MSE for test set [171]  is [0.]\n",
      "MSE for test set [172]  is [0.]\n",
      "MSE for test set [173]  is [1.]\n",
      "MSE for test set [174]  is [0.]\n",
      "MSE for test set [175]  is [0.]\n",
      "MSE for test set [176]  is [0.]\n",
      "MSE for test set [177]  is [0.]\n",
      "MSE for test set [178]  is [0.]\n",
      "MSE for test set [179]  is [1.]\n",
      "MSE for test set [180]  is [0.]\n",
      "MSE for test set [181]  is [0.]\n",
      "MSE for test set [182]  is [0.]\n",
      "MSE for test set [183]  is [0.]\n",
      "MSE for test set [184]  is [0.]\n",
      "MSE for test set [185]  is [1.]\n",
      "MSE for test set [186]  is [1.]\n",
      "MSE for test set [187]  is [0.]\n",
      "MSE for test set [188]  is [0.]\n",
      "MSE for test set [189]  is [0.]\n",
      "MSE for test set [190]  is [0.]\n",
      "MSE for test set [191]  is [0.]\n",
      "MSE for test set [192]  is [1.]\n",
      "MSE for test set [193]  is [0.]\n",
      "MSE for test set [194]  is [0.]\n",
      "MSE for test set [195]  is [0.]\n",
      "MSE for test set [196]  is [1.]\n",
      "MSE for test set [197]  is [0.]\n",
      "MSE for test set [198]  is [0.]\n",
      "MSE for test set [199]  is [0.]\n",
      "MSE for test set [200]  is [0.]\n",
      "MSE for test set [201]  is [1.]\n",
      "MSE for test set [202]  is [0.]\n",
      "MSE for test set [203]  is [0.]\n",
      "MSE for test set [204]  is [0.]\n",
      "MSE for test set [205]  is [0.]\n",
      "MSE for test set [206]  is [0.]\n",
      "MSE for test set [207]  is [0.]\n",
      "MSE for test set [208]  is [0.]\n",
      "MSE for test set [209]  is [0.]\n",
      "MSE for test set [210]  is [0.]\n",
      "MSE for test set [211]  is [1.]\n",
      "MSE for test set [212]  is [0.]\n",
      "MSE for test set [213]  is [0.]\n",
      "MSE for test set [214]  is [1.]\n",
      "MSE for test set [215]  is [1.]\n",
      "MSE for test set [216]  is [0.]\n",
      "MSE for test set [217]  is [0.]\n",
      "MSE for test set [218]  is [0.]\n",
      "MSE for test set [219]  is [1.]\n",
      "MSE for test set [220]  is [1.]\n",
      "MSE for test set [221]  is [1.]\n",
      "MSE for test set [222]  is [1.]\n",
      "MSE for test set [223]  is [0.]\n",
      "MSE for test set [224]  is [0.]\n",
      "MSE for test set [225]  is [0.]\n",
      "MSE for test set [226]  is [1.]\n",
      "MSE for test set [227]  is [1.]\n",
      "MSE for test set [228]  is [1.]\n",
      "MSE for test set [229]  is [0.]\n",
      "MSE for test set [230]  is [1.]\n",
      "MSE for test set [231]  is [1.]\n",
      "MSE for test set [232]  is [0.]\n",
      "MSE for test set [233]  is [1.]\n",
      "MSE for test set [234]  is [1.]\n",
      "MSE for test set [235]  is [0.]\n",
      "MSE for test set [236]  is [0.]\n",
      "MSE for test set [237]  is [1.]\n",
      "MSE for test set [238]  is [1.]\n",
      "MSE for test set [239]  is [0.]\n",
      "MSE for test set [240]  is [0.]\n",
      "MSE for test set [241]  is [0.]\n",
      "MSE for test set [242]  is [0.]\n",
      "MSE for test set [243]  is [0.]\n",
      "MSE for test set [244]  is [0.]\n",
      "MSE for test set [245]  is [0.]\n",
      "MSE for test set [246]  is [0.]\n",
      "MSE for test set [247]  is [1.]\n",
      "MSE for test set [248]  is [0.]\n",
      "MSE for test set [249]  is [0.]\n",
      "MSE for test set [250]  is [1.]\n",
      "MSE for test set [251]  is [1.]\n",
      "MSE for test set [252]  is [1.]\n",
      "MSE for test set [253]  is [0.]\n",
      "MSE for test set [254]  is [0.]\n",
      "MSE for test set [255]  is [0.]\n",
      "MSE for test set [256]  is [1.]\n",
      "MSE for test set [257]  is [0.]\n",
      "MSE for test set [258]  is [0.]\n",
      "MSE for test set [259]  is [0.]\n",
      "MSE for test set [260]  is [0.]\n",
      "MSE for test set [261]  is [0.]\n",
      "MSE for test set [262]  is [0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set [263]  is [0.]\n",
      "MSE for test set [264]  is [0.]\n",
      "MSE for test set [265]  is [0.]\n",
      "MSE for test set [266]  is [0.]\n",
      "MSE for test set [267]  is [0.]\n",
      "MSE for test set [268]  is [0.]\n",
      "MSE for test set [269]  is [0.]\n",
      "MSE for test set [270]  is [0.]\n",
      "MSE for test set [271]  is [1.]\n",
      "MSE for test set [272]  is [1.]\n",
      "MSE for test set [273]  is [1.]\n",
      "MSE for test set [274]  is [1.]\n",
      "MSE for test set [275]  is [0.]\n",
      "MSE for test set [276]  is [0.]\n",
      "MSE for test set [277]  is [0.]\n",
      "MSE for test set [278]  is [0.]\n",
      "MSE for test set [279]  is [1.]\n",
      "MSE for test set [280]  is [0.]\n",
      "MSE for test set [281]  is [0.]\n",
      "MSE for test set [282]  is [0.]\n",
      "MSE for test set [283]  is [0.]\n",
      "MSE for test set [284]  is [0.]\n",
      "MSE for test set [285]  is [0.]\n",
      "MSE for test set [286]  is [0.]\n",
      "MSE for test set [287]  is [0.]\n",
      "MSE for test set [288]  is [0.]\n",
      "MSE for test set [289]  is [0.]\n",
      "MSE for test set [290]  is [0.]\n",
      "MSE for test set [291]  is [1.]\n",
      "MSE for test set [292]  is [0.]\n",
      "MSE for test set [293]  is [0.]\n",
      "test estimate MSE loocv= 0.3197278911564626 , test estimate MSE standard err= 0.4663710612517717\n"
     ]
    }
   ],
   "source": [
    "N_loo = Xvals.shape[0]\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(Xvals)\n",
    "MSE_vec = np.zeros(N_loo)\n",
    "y_pred_vals = []\n",
    "\n",
    "# This loop takes several minutes\n",
    "for train_index, test_index in loo.split(Xvals):\n",
    "    X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    clf_nn = sklearn.neural_network.MLPClassifier()\n",
    "    clf_nn.fit(np.stack(X_train, axis=0), y_train)\n",
    "    y_pred = clf_nn.predict(np.stack(X_test, axis=0))\n",
    "    y_pred_vals.append(y_pred)\n",
    "    MSE_vec[test_index] = (y_test - y_pred) ** 2\n",
    "    print('MSE for test set', test_index, ' is', MSE_vec[test_index])\n",
    "\n",
    "MSE_loo = MSE_vec.mean()\n",
    "MSE_loo_std = MSE_vec.std()\n",
    "print('test estimate MSE loocv=', MSE_loo,\n",
    "      ', test estimate MSE standard err=', MSE_loo_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.69      0.68       143\n",
      "          1       0.69      0.68      0.68       151\n",
      "\n",
      "avg / total       0.68      0.68      0.68       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yvals, np.asarray(y_pred_vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super great, but maybe it doesn't matter? As long as it works for more recent releases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set [0]  is [0.]\n",
      "MSE for test set [1]  is [0.]\n",
      "MSE for test set [2]  is [0.]\n",
      "MSE for test set [3]  is [0.]\n",
      "MSE for test set [4]  is [0.]\n",
      "MSE for test set [5]  is [0.]\n",
      "MSE for test set [6]  is [0.]\n",
      "MSE for test set [7]  is [0.]\n",
      "MSE for test set [8]  is [0.]\n",
      "MSE for test set [9]  is [0.]\n",
      "MSE for test set [10]  is [0.]\n",
      "MSE for test set [11]  is [0.]\n",
      "MSE for test set [12]  is [0.]\n",
      "MSE for test set [13]  is [0.]\n",
      "MSE for test set [14]  is [0.]\n",
      "MSE for test set [15]  is [1.]\n",
      "MSE for test set [16]  is [1.]\n",
      "MSE for test set [17]  is [1.]\n",
      "MSE for test set [18]  is [1.]\n",
      "MSE for test set [19]  is [1.]\n",
      "MSE for test set [20]  is [1.]\n",
      "MSE for test set [21]  is [1.]\n",
      "MSE for test set [22]  is [0.]\n",
      "MSE for test set [23]  is [1.]\n",
      "MSE for test set [24]  is [1.]\n",
      "MSE for test set [25]  is [1.]\n",
      "MSE for test set [26]  is [1.]\n",
      "MSE for test set [27]  is [1.]\n",
      "MSE for test set [28]  is [1.]\n",
      "MSE for test set [29]  is [1.]\n",
      "MSE for test set [30]  is [1.]\n",
      "MSE for test set [31]  is [1.]\n",
      "MSE for test set [32]  is [1.]\n",
      "MSE for test set [33]  is [1.]\n",
      "MSE for test set [34]  is [1.]\n",
      "MSE for test set [35]  is [1.]\n",
      "MSE for test set [36]  is [1.]\n",
      "MSE for test set [37]  is [0.]\n",
      "MSE for test set [38]  is [0.]\n",
      "MSE for test set [39]  is [1.]\n",
      "MSE for test set [40]  is [0.]\n",
      "MSE for test set [41]  is [1.]\n",
      "MSE for test set [42]  is [0.]\n",
      "MSE for test set [43]  is [1.]\n",
      "MSE for test set [44]  is [1.]\n",
      "MSE for test set [45]  is [0.]\n",
      "MSE for test set [46]  is [1.]\n",
      "MSE for test set [47]  is [0.]\n",
      "MSE for test set [48]  is [1.]\n",
      "MSE for test set [49]  is [1.]\n",
      "MSE for test set [50]  is [1.]\n",
      "MSE for test set [51]  is [0.]\n",
      "MSE for test set [52]  is [0.]\n",
      "MSE for test set [53]  is [1.]\n",
      "MSE for test set [54]  is [0.]\n",
      "MSE for test set [55]  is [1.]\n",
      "MSE for test set [56]  is [1.]\n",
      "MSE for test set [57]  is [0.]\n",
      "MSE for test set [58]  is [0.]\n",
      "MSE for test set [59]  is [0.]\n",
      "MSE for test set [60]  is [1.]\n",
      "MSE for test set [61]  is [1.]\n",
      "MSE for test set [62]  is [0.]\n",
      "MSE for test set [63]  is [1.]\n",
      "MSE for test set [64]  is [0.]\n",
      "MSE for test set [65]  is [0.]\n",
      "MSE for test set [66]  is [0.]\n",
      "MSE for test set [67]  is [0.]\n",
      "MSE for test set [68]  is [1.]\n",
      "MSE for test set [69]  is [0.]\n",
      "MSE for test set [70]  is [1.]\n",
      "MSE for test set [71]  is [0.]\n",
      "MSE for test set [72]  is [0.]\n",
      "MSE for test set [73]  is [0.]\n",
      "MSE for test set [74]  is [0.]\n",
      "MSE for test set [75]  is [0.]\n",
      "MSE for test set [76]  is [0.]\n",
      "MSE for test set [77]  is [0.]\n",
      "MSE for test set [78]  is [1.]\n",
      "MSE for test set [79]  is [0.]\n",
      "MSE for test set [80]  is [1.]\n",
      "MSE for test set [81]  is [1.]\n",
      "MSE for test set [82]  is [1.]\n",
      "MSE for test set [83]  is [0.]\n",
      "MSE for test set [84]  is [0.]\n",
      "MSE for test set [85]  is [0.]\n",
      "MSE for test set [86]  is [0.]\n",
      "MSE for test set [87]  is [0.]\n",
      "MSE for test set [88]  is [0.]\n",
      "MSE for test set [89]  is [0.]\n",
      "MSE for test set [90]  is [0.]\n",
      "MSE for test set [91]  is [0.]\n",
      "MSE for test set [92]  is [0.]\n",
      "MSE for test set [93]  is [0.]\n",
      "MSE for test set [94]  is [0.]\n",
      "MSE for test set [95]  is [0.]\n",
      "MSE for test set [96]  is [1.]\n",
      "MSE for test set [97]  is [1.]\n",
      "MSE for test set [98]  is [1.]\n",
      "MSE for test set [99]  is [1.]\n",
      "MSE for test set [100]  is [0.]\n",
      "MSE for test set [101]  is [0.]\n",
      "MSE for test set [102]  is [0.]\n",
      "MSE for test set [103]  is [0.]\n",
      "MSE for test set [104]  is [1.]\n",
      "MSE for test set [105]  is [1.]\n",
      "MSE for test set [106]  is [0.]\n",
      "MSE for test set [107]  is [0.]\n",
      "MSE for test set [108]  is [1.]\n",
      "MSE for test set [109]  is [1.]\n",
      "MSE for test set [110]  is [1.]\n",
      "MSE for test set [111]  is [1.]\n",
      "MSE for test set [112]  is [1.]\n",
      "MSE for test set [113]  is [0.]\n",
      "MSE for test set [114]  is [0.]\n",
      "MSE for test set [115]  is [1.]\n",
      "MSE for test set [116]  is [0.]\n",
      "MSE for test set [117]  is [0.]\n",
      "MSE for test set [118]  is [0.]\n",
      "MSE for test set [119]  is [1.]\n",
      "MSE for test set [120]  is [1.]\n",
      "MSE for test set [121]  is [1.]\n",
      "MSE for test set [122]  is [0.]\n",
      "MSE for test set [123]  is [0.]\n",
      "MSE for test set [124]  is [1.]\n",
      "MSE for test set [125]  is [0.]\n",
      "MSE for test set [126]  is [0.]\n",
      "MSE for test set [127]  is [0.]\n",
      "MSE for test set [128]  is [0.]\n",
      "MSE for test set [129]  is [0.]\n",
      "MSE for test set [130]  is [0.]\n",
      "MSE for test set [131]  is [0.]\n",
      "MSE for test set [132]  is [0.]\n",
      "MSE for test set [133]  is [0.]\n",
      "MSE for test set [134]  is [0.]\n",
      "MSE for test set [135]  is [0.]\n",
      "MSE for test set [136]  is [0.]\n",
      "MSE for test set [137]  is [0.]\n",
      "MSE for test set [138]  is [0.]\n",
      "MSE for test set [139]  is [0.]\n",
      "MSE for test set [140]  is [0.]\n",
      "MSE for test set [141]  is [0.]\n",
      "MSE for test set [142]  is [0.]\n",
      "MSE for test set [143]  is [0.]\n",
      "MSE for test set [144]  is [0.]\n",
      "MSE for test set [145]  is [0.]\n",
      "MSE for test set [146]  is [0.]\n",
      "MSE for test set [147]  is [0.]\n",
      "MSE for test set [148]  is [0.]\n",
      "MSE for test set [149]  is [0.]\n",
      "MSE for test set [150]  is [0.]\n",
      "MSE for test set [151]  is [0.]\n",
      "MSE for test set [152]  is [0.]\n",
      "MSE for test set [153]  is [0.]\n",
      "MSE for test set [154]  is [0.]\n",
      "MSE for test set [155]  is [0.]\n",
      "MSE for test set [156]  is [0.]\n",
      "MSE for test set [157]  is [0.]\n",
      "MSE for test set [158]  is [0.]\n",
      "MSE for test set [159]  is [0.]\n",
      "MSE for test set [160]  is [0.]\n",
      "MSE for test set [161]  is [1.]\n",
      "MSE for test set [162]  is [1.]\n",
      "MSE for test set [163]  is [0.]\n",
      "MSE for test set [164]  is [1.]\n",
      "MSE for test set [165]  is [1.]\n",
      "MSE for test set [166]  is [1.]\n",
      "MSE for test set [167]  is [0.]\n",
      "MSE for test set [168]  is [0.]\n",
      "MSE for test set [169]  is [0.]\n",
      "MSE for test set [170]  is [1.]\n",
      "MSE for test set [171]  is [0.]\n",
      "MSE for test set [172]  is [0.]\n",
      "MSE for test set [173]  is [1.]\n",
      "MSE for test set [174]  is [0.]\n",
      "MSE for test set [175]  is [0.]\n",
      "MSE for test set [176]  is [0.]\n",
      "MSE for test set [177]  is [1.]\n",
      "MSE for test set [178]  is [1.]\n",
      "MSE for test set [179]  is [0.]\n",
      "MSE for test set [180]  is [0.]\n",
      "MSE for test set [181]  is [1.]\n",
      "MSE for test set [182]  is [0.]\n",
      "MSE for test set [183]  is [0.]\n",
      "MSE for test set [184]  is [0.]\n",
      "MSE for test set [185]  is [1.]\n",
      "MSE for test set [186]  is [1.]\n",
      "MSE for test set [187]  is [0.]\n",
      "MSE for test set [188]  is [0.]\n",
      "MSE for test set [189]  is [0.]\n",
      "MSE for test set [190]  is [0.]\n",
      "MSE for test set [191]  is [0.]\n",
      "MSE for test set [192]  is [1.]\n",
      "MSE for test set [193]  is [0.]\n",
      "MSE for test set [194]  is [0.]\n",
      "MSE for test set [195]  is [0.]\n",
      "MSE for test set [196]  is [1.]\n",
      "MSE for test set [197]  is [0.]\n",
      "MSE for test set [198]  is [0.]\n",
      "MSE for test set [199]  is [1.]\n",
      "MSE for test set [200]  is [0.]\n",
      "MSE for test set [201]  is [0.]\n",
      "MSE for test set [202]  is [0.]\n",
      "MSE for test set [203]  is [0.]\n",
      "MSE for test set [204]  is [0.]\n",
      "MSE for test set [205]  is [0.]\n",
      "MSE for test set [206]  is [0.]\n",
      "MSE for test set [207]  is [1.]\n",
      "MSE for test set [208]  is [1.]\n",
      "MSE for test set [209]  is [0.]\n",
      "MSE for test set [210]  is [0.]\n",
      "MSE for test set [211]  is [1.]\n",
      "MSE for test set [212]  is [0.]\n",
      "MSE for test set [213]  is [0.]\n",
      "MSE for test set [214]  is [1.]\n",
      "MSE for test set [215]  is [1.]\n",
      "MSE for test set [216]  is [0.]\n",
      "MSE for test set [217]  is [0.]\n",
      "MSE for test set [218]  is [0.]\n",
      "MSE for test set [219]  is [0.]\n",
      "MSE for test set [220]  is [1.]\n",
      "MSE for test set [221]  is [1.]\n",
      "MSE for test set [222]  is [1.]\n",
      "MSE for test set [223]  is [0.]\n",
      "MSE for test set [224]  is [0.]\n",
      "MSE for test set [225]  is [0.]\n",
      "MSE for test set [226]  is [1.]\n",
      "MSE for test set [227]  is [1.]\n",
      "MSE for test set [228]  is [1.]\n",
      "MSE for test set [229]  is [0.]\n",
      "MSE for test set [230]  is [1.]\n",
      "MSE for test set [231]  is [0.]\n",
      "MSE for test set [232]  is [0.]\n",
      "MSE for test set [233]  is [1.]\n",
      "MSE for test set [234]  is [1.]\n",
      "MSE for test set [235]  is [0.]\n",
      "MSE for test set [236]  is [0.]\n",
      "MSE for test set [237]  is [1.]\n",
      "MSE for test set [238]  is [1.]\n",
      "MSE for test set [239]  is [0.]\n",
      "MSE for test set [240]  is [0.]\n",
      "MSE for test set [241]  is [1.]\n",
      "MSE for test set [242]  is [0.]\n",
      "MSE for test set [243]  is [0.]\n",
      "MSE for test set [244]  is [0.]\n",
      "MSE for test set [245]  is [0.]\n",
      "MSE for test set [246]  is [0.]\n",
      "MSE for test set [247]  is [0.]\n",
      "MSE for test set [248]  is [0.]\n",
      "MSE for test set [249]  is [0.]\n",
      "MSE for test set [250]  is [1.]\n",
      "MSE for test set [251]  is [1.]\n",
      "MSE for test set [252]  is [1.]\n",
      "MSE for test set [253]  is [1.]\n",
      "MSE for test set [254]  is [1.]\n",
      "MSE for test set [255]  is [1.]\n",
      "MSE for test set [256]  is [1.]\n",
      "MSE for test set [257]  is [0.]\n",
      "MSE for test set [258]  is [0.]\n",
      "MSE for test set [259]  is [0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for test set [260]  is [0.]\n",
      "MSE for test set [261]  is [0.]\n",
      "MSE for test set [262]  is [0.]\n",
      "MSE for test set [263]  is [0.]\n",
      "MSE for test set [264]  is [0.]\n",
      "MSE for test set [265]  is [0.]\n",
      "MSE for test set [266]  is [0.]\n",
      "MSE for test set [267]  is [0.]\n",
      "MSE for test set [268]  is [1.]\n",
      "MSE for test set [269]  is [0.]\n",
      "MSE for test set [270]  is [0.]\n",
      "MSE for test set [271]  is [1.]\n",
      "MSE for test set [272]  is [0.]\n",
      "MSE for test set [273]  is [1.]\n",
      "MSE for test set [274]  is [0.]\n",
      "MSE for test set [275]  is [0.]\n",
      "MSE for test set [276]  is [0.]\n",
      "MSE for test set [277]  is [0.]\n",
      "MSE for test set [278]  is [0.]\n",
      "MSE for test set [279]  is [1.]\n",
      "MSE for test set [280]  is [0.]\n",
      "MSE for test set [281]  is [0.]\n",
      "MSE for test set [282]  is [0.]\n",
      "MSE for test set [283]  is [0.]\n",
      "MSE for test set [284]  is [0.]\n",
      "MSE for test set [285]  is [0.]\n",
      "MSE for test set [286]  is [0.]\n",
      "MSE for test set [287]  is [0.]\n",
      "MSE for test set [288]  is [0.]\n",
      "MSE for test set [289]  is [0.]\n",
      "MSE for test set [290]  is [0.]\n",
      "MSE for test set [291]  is [0.]\n",
      "MSE for test set [292]  is [0.]\n",
      "MSE for test set [293]  is [0.]\n",
      "test estimate MSE loocv= 0.3401360544217687 , test estimate MSE standard err= 0.4737547033055824\n"
     ]
    }
   ],
   "source": [
    "y_pred_vals2 = []\n",
    "#this loop takes several minutes\n",
    "for train_index, test_index in loo.split(Xvals):\n",
    "    X_train, X_test = Xvals[train_index], Xvals[test_index]\n",
    "    y_train, y_test = yvals[train_index], yvals[test_index]\n",
    "    tree = sklearn.tree.DecisionTreeClassifier(max_depth=10) #Create an instance of our decision tree classifier.\n",
    "    bag = sklearn.ensemble.BaggingClassifier(tree, n_estimators=100, max_samples=0.8, random_state=1) #Each tree uses up to 80% of the data\n",
    "    bag.fit(np.stack(X_train, axis =0), y_train) #Fit the bagged classifier\n",
    "    y_pred = bag.predict(np.stack(X_test, axis=0))\n",
    "    y_pred_vals2.append(y_pred)\n",
    "    MSE_vec[test_index] = (y_test - y_pred) ** 2\n",
    "    print('MSE for test set', test_index, ' is', MSE_vec[test_index])\n",
    "\n",
    "MSE_loo = MSE_vec.mean()\n",
    "MSE_loo_std = MSE_vec.std()\n",
    "print('test estimate MSE loocv=', MSE_loo,\n",
    "      ', test estimate MSE standard err=', MSE_loo_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.52      0.60       143\n",
      "          1       0.64      0.79      0.70       151\n",
      "\n",
      "avg / total       0.67      0.66      0.65       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yvals, np.asarray(y_pred_vals2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
